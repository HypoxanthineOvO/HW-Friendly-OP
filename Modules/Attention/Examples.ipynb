{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a224a016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7cdd60ee39b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Fixed the random seed\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e64550b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scaled_Product(Q: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n",
    "    # Compute QK^T\n",
    "    QKT = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    print(f\"QKT: {QKT}\")\n",
    "    # Scale QK^T\n",
    "    d_k = K.size(-1)\n",
    "    QKT_scaled = QKT / (d_k ** 0.5)\n",
    "    \n",
    "    # Apply softmax to get attention weights\n",
    "    attention_weights = torch.nn.functional.softmax(QKT_scaled, dim=-1)\n",
    "    return attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "483c4563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Approx_Scale_Product(Q: torch.Tensor, K: torch.Tensor, max_iter: int) -> torch.Tensor:\n",
    "    assert Q.dim() == 2 and K.dim() == 2, \"Q and K must be 2D tensors\"\n",
    "    assert Q.size(1) == K.size(1), \"Q and K must have the same feature dimension\"\n",
    "    Q_vec_size = Q.size(0)\n",
    "    dtype, device = Q.dtype, Q.device\n",
    "    \n",
    "    total_score = []\n",
    "    \n",
    "    for q_id in range(Q_vec_size):\n",
    "        q_mat = Q[q_id].repeat(K.size(0), 1)\n",
    "        product = K * q_mat\n",
    "        product_for_max = product.clone()\n",
    "        product_for_min = product.clone()\n",
    "        greedy_score = torch.zeros(K.size(0), dtype=dtype, device=device)\n",
    "        print(f\"Product: {product}\")\n",
    "        for iter in range(max_iter):\n",
    "            max_val = product_for_max.max()\n",
    "            max_idx_flatten = product_for_max.argmax().item()\n",
    "            max_idx_row, max_idx_col = divmod(max_idx_flatten, product_for_max.size(1))\n",
    "            print(f\"Iteration {iter}: max_val: {max_val:.2f}, max_idx: {max_idx_row}\", end = \" | \")\n",
    "            \n",
    "            min_val = product_for_min.min()\n",
    "            min_idx_flatten = product_for_min.argmin().item()\n",
    "            min_idx_row, min_idx_col = divmod(min_idx_flatten, product_for_min.size(1))\n",
    "            print(f\"min_val: {min_val:.2f}, min_idx: {min_idx_row}\")\n",
    "            # Update the greedy score\n",
    "            greedy_score[max_idx_row] += max_val\n",
    "            product_for_max[max_idx_row, max_idx_col] = float('-inf')  # Set the max value to -inf to avoid selecting it again\n",
    "            greedy_score[min_idx_row] += min_val\n",
    "            product_for_min[min_idx_row, min_idx_col] = float('inf')  # Set the min value to inf to avoid selecting it again\n",
    "            print(f\"Greedy Score: {greedy_score}\")\n",
    "        # Set the <0 values to 0\n",
    "        greedy_score[greedy_score < 0] = 0\n",
    "        total_score.append(greedy_score)\n",
    "    total_score = torch.stack(total_score)\n",
    "    print(f\"Approx QKT: {total_score}\")\n",
    "    d_k = K.size(-1)  # Dimension of keys (D)\n",
    "    total_score =  total_score/ (d_k ** 0.5)\n",
    "    \n",
    "    results = torch.nn.functional.softmax(total_score, dim=-1)\n",
    "    \n",
    "    # Step 5: Scale the greedy score\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86973c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QKT: tensor([[-0.1900, -0.2200,  0.7400,  0.3900]])\n",
      "Product: tensor([[-0.4800, -0.0300,  0.3200],\n",
      "        [ 0.0800,  0.0600, -0.3600],\n",
      "        [ 0.6400, -0.1800,  0.2800],\n",
      "        [ 0.4000, -0.2100,  0.2000]])\n",
      "Iteration 0: max_val: 0.64, max_idx: 2 | min_val: -0.48, min_idx: 0\n",
      "Greedy Score: tensor([-0.4800,  0.0000,  0.6400,  0.0000])\n",
      "Iteration 1: max_val: 0.40, max_idx: 3 | min_val: -0.36, min_idx: 1\n",
      "Greedy Score: tensor([-0.4800, -0.3600,  0.6400,  0.4000])\n",
      "Iteration 2: max_val: 0.32, max_idx: 0 | min_val: -0.21, min_idx: 3\n",
      "Greedy Score: tensor([-0.1600, -0.3600,  0.6400,  0.1900])\n",
      "Iteration 3: max_val: 0.28, max_idx: 2 | min_val: -0.18, min_idx: 2\n",
      "Greedy Score: tensor([-0.1600, -0.3600,  0.7400,  0.1900])\n",
      "Iteration 4: max_val: 0.20, max_idx: 3 | min_val: -0.03, min_idx: 0\n",
      "Greedy Score: tensor([-0.1900, -0.3600,  0.7400,  0.3900])\n",
      "Iteration 5: max_val: 0.08, max_idx: 1 | min_val: 0.06, min_idx: 1\n",
      "Greedy Score: tensor([-0.1900, -0.2200,  0.7400,  0.3900])\n",
      "Approx QKT: tensor([[0.0000, 0.0000, 0.7400, 0.3900]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1964, 0.1930, 0.3360, 0.2745]]),\n",
       " tensor([[0.2090, 0.2090, 0.3203, 0.2617]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = torch.tensor([\n",
    "    [0.8, -0.3, 0.4],\n",
    "])\n",
    "K = torch.tensor([\n",
    "    [-0.6, 0.1, 0.8],\n",
    "    [0.1, -0.2, -0.9],\n",
    "    [0.8, 0.6, 0.7],\n",
    "    [0.5, 0.7, 0.5]\n",
    "])\n",
    "max_iter = 6\n",
    "\n",
    "reference = Scaled_Product(Q, K)\n",
    "ours = Approx_Scale_Product(Q, K, max_iter)\n",
    "reference, ours\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
